from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit

# Initialize SparkSession
spark = SparkSession.builder.master("local").appName("Hierarchy Levels").getOrCreate()

# Sample data
data = [
    (1, "Alice", None),  # Root employee
    (2, "Bob", 1),       # Reports to Alice
    (3, "Charlie", 2),   # Reports to Bob
    (4, "David", 1),     # Reports to Alice
    (5, "Eve", 3)        # Reports to Charlie
]

columns = ["userId", "userName", "managerId"]

# Create DataFrame
df = spark.createDataFrame(data, columns)

# Initialize root employee, set EmpLevel to 0
df_levels = df.filter(col("managerId").isNull()).withColumn("EmpLevel", lit(0))

# Create a DataFrame to hold the results as they are built up
final_df = df_levels

# Iteratively process subordinates
max_iterations = 10  # Adjust based on expected maximum depth
has_new_levels = True

for _ in range(max_iterations):
    if not has_new_levels:
        break
    
    # Identify subordinates of employees whose levels have already been calculated
    df_new_levels = (
        df.alias("emp")
        .join(df_levels.alias("mgr"), col("emp.managerId") == col("mgr.userId"), "inner")
        .select(
            col("emp.userId"),
            col("emp.userName"),
            col("emp.managerId"),
            (col("mgr.EmpLevel") + 1).alias("EmpLevel")
        )
    )
    
    # If no new levels are calculated, set the flag to false to break the loop
    if df_new_levels.rdd.isEmpty():
        has_new_levels = False
    else:
        # Append the new levels to the final result set
        final_df = final_df.union(df_new_levels)
        
        # Update df_levels to include only the newly calculated levels for the next iteration
        df_levels = df_new_levels

# Final result
final_df.orderBy("EmpLevel", "userId").show()





i = 1
check_date = '2010-12-23'
start_product_id = 972 # provide a specific id 
# bill_df corresponds to the "BOM_CTE" clause in the above query
df = spark.sql(f"""
SELECT b.ProductAssemblyID
  , b.ComponentID
  , p.Name
  , b.PerAssemblyQty
  , p.StandardCost
  , p.ListPrice
  , b.BOMLevel
  , 0 as RecursionLevel 
FROM BillOfMaterials b
    INNER JOIN Product p ON b.ComponentID = p.ProductID
WHERE b.ProductAssemblyID = {start_product_id} AND '{check_date}' >= b.StartDate AND '{check_date}' <= IFNULL(b.EndDate, '{check_date}')
""")

# this view is our 'CTE' that we reference with each pass
df.createOrReplaceTempView('recursion_df')

while True:
  # select data for this recursion level
  bill_df = spark.sql(f"""
  SELECT b.ProductAssemblyID
    , b.ComponentID
    , p.Name
    , b.PerAssemblyQty
    , p.StandardCost
    , p.ListPrice
    , b.BOMLevel
    , {i} as RecursionLevel 
  FROM recursion_df cte
      INNER JOIN BillOfMaterials b ON b.ProductAssemblyID = cte.ComponentID
      INNER JOIN Product p ON b.ComponentID = p.ProductID
  WHERE '{check_date}' >= b.StartDate AND '{check_date}' <= IFNULL(b.EndDate, '{check_date}')
  """)
  
  # this view is our 'CTE' that we reference with each pass
  bill_df.createOrReplaceTempView('recursion_df')
  # add the results to the main output dataframe
  df = df.union(bill_df)
  # if there are no results at this recursion level then break
  if bill_df.count() == 0:
      df.createOrReplaceTempView("final_df")
      break
  else:
      i += 1


























https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/pipelineruns/{runId}/activityruns?api-version=2018-06-01



import requests

# Azure AD and ADF configurations
tenant_id = "<YOUR_TENANT_ID>"
client_id = "<YOUR_CLIENT_ID>"
client_secret = "<YOUR_CLIENT_SECRET>"
subscription_id = "<YOUR_SUBSCRIPTION_ID>"
resource_group_name = "<YOUR_RESOURCE_GROUP_NAME>"
factory_name = "<YOUR_FACTORY_NAME>"
pipeline_name = "<YOUR_PIPELINE_NAME>"

# 1. Get the OAuth2 Token
def get_access_token(tenant_id, client_id, client_secret):
    url = f"https://login.microsoftonline.com/{tenant_id}/oauth2/token"
    payload = {
        "grant_type": "client_credentials",
        "client_id": client_id,
        "client_secret": client_secret,
        "resource": "https://management.azure.com/"
    }
    
    response = requests.post(url, data=payload)
    
    if response.status_code == 200:
        return response.json().get("access_token")
    else:
        raise Exception(f"Failed to get access token: {response.text}")

# 2. Get Pipeline Runs
def get_pipeline_runs(subscription_id, resource_group_name, factory_name, access_token):
    url = f"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/providers/Microsoft.DataFactory/factories/{factory_name}/pipelineruns?api-version=2018-06-01"
    
    headers = {
        "Authorization": f"Bearer {access_token}"
    }
    
    response = requests.get(url, headers=headers)
    
    if response.status_code == 200:
        return response.json().get("value", [])
    else:
        raise Exception(f"Failed to get pipeline runs: {response.text}")

# 3. Get Activity Runs by Pipeline Run ID
def get_activity_runs(subscription_id, resource_group_name, factory_name, run_id, access_token):
    url = f"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/providers/Microsoft.DataFactory/factories/{factory_name}/pipelineruns/{run_id}/activityruns?api-version=2018-06-01"
    
    headers = {
        "Authorization": f"Bearer {access_token}"
    }
    
    response = requests.get(url, headers=headers)
    
    if response.status_code == 200:
        return response.json().get("value", [])
    else:
        raise Exception(f"Failed to get activity runs: {response.text}")

# Main function
if __name__ == "__main__":
    try:
        # Step 1: Get the access token
        access_token = get_access_token(tenant_id, client_id, client_secret)
        print("Access token acquired.")
        
        # Step 2: Fetch pipeline runs
        pipeline_runs = get_pipeline_runs(subscription_id, resource_group_name, factory_name, access_token)
        
        for run in pipeline_runs:
            run_id = run.get("runId")
            pipeline_name = run.get("pipelineName")
            print(f"Pipeline Name: {pipeline_name}, Run ID: {run_id}")
            
            # Step 3: Fetch activity runs for each pipeline run
            activity_runs = get_activity_runs(subscription_id, resource_group_name, factory_name, run_id, access_token)
            
            for activity in activity_runs:
                print(f"Activity Name: {activity.get('activityName')}, Activity Type: {activity.get('activityType')}, Status: {activity.get('status')}")
                
    except Exception as e:
        print(f"Error: {str(e)}")
